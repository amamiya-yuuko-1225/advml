{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Assignment\n",
    "\n",
    "This assignment should be done in groups of 2 or 3 and consists of a number of implementation and theory problems based on the topics discussed in the lectures and the course literature (specifically, **version 5** on arXiv):\n",
    "\n",
    "[Bandits] *Aleksandrs Slivkins, [Introduction to Multi-Armed Bandits](https://arxiv.org/pdf/1904.07272v5.pdf), Found. Trends Mach. Learn. 12(1-2): 1-286 (2019)*\n",
    "\n",
    "In the implementation problems **(1, 2, 3 and 5)**, you will implement multi-armed bandit algorithms from the [Bandits] book and use them in a provided multi-armed bandit environment. These problems will be graded based on the correctness of the code. You are only required to implement 3 out of 4 of these algorithms and will receive at most 6 points from the implementations. Points may be deducted for incorrect implementation of the 4th algorithm.\n",
    "\n",
    "In the theory problems **(4 and 6)**, you will derive some properties of the algorithms. These problems will be graded based on the correctness of the arguments.\n",
    "\n",
    "In the applied problem **(7)**, you will apply the implemented algorithms or another algorithm of your choosing to a bandit problem with irregular real-world data. This problem will be graded based on your algorithm's performance relative to a set of baselines. This problem likely requires you to revise your previously implemented algorithms or implement new algorithms.\n",
    "\n",
    "You may use the python libraries imported below (*numpy*, *scipy.stats* and *pandas*).\n",
    "\n",
    "The assignment should be handed in as an updated notebook. The entire notebook should be run before it is handed in, so that the plots are visible. Ensure that it is completely runnable, in the case that we want to reproduce the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains imports. It may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "SEED = 150\n",
    "ITERATIONS = 20\n",
    "K = 100\n",
    "K_CROPS = 3\n",
    "T = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the bandit environment and may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "class Environment:\n",
    "    def __init__(self, K: int=10, seed: int=0):\n",
    "        self.random_state = np.random.RandomState(seed=seed)\n",
    "        self.mu = st.beta.rvs(a=1, b=1, size=K, random_state=self.random_state)\n",
    "        \n",
    "    def expected_value(self, a: int) -> float:\n",
    "        return self.mu[a]\n",
    "        \n",
    "    def perform_action(self, a: int) -> int:\n",
    "        return st.bernoulli.rvs(self.mu[a], random_state=self.random_state)\n",
    "        \n",
    "    def optimal_action(self) -> np.intp:\n",
    "        return np.argmax(self.mu)\n",
    "\n",
    "class CropEnvironment(Environment):\n",
    "    def __init__(self, seed: int = 0):\n",
    "        with open('samples_small.pkl', 'rb') as f:\n",
    "            samples: np.ndarray = pickle.load(f)\n",
    "        self.K = samples.shape[0]\n",
    "        self.N = samples.shape[1]\n",
    "        samples = samples / samples.max()\n",
    "        self.random_state = np.random.default_rng(seed=seed)\n",
    "        self.shuffled_samples = self.random_state.permutation(samples, axis = 0)\n",
    "        self.mu = np.mean(self.shuffled_samples, axis = 1)\n",
    "\n",
    "    def expected_value(self, a: int) -> float:\n",
    "        return self.mu[a]\n",
    "        \n",
    "    def perform_action(self, a: int) -> float:\n",
    "        i = self.random_state.integers(0, self.N)\n",
    "        return self.shuffled_samples[a,i]\n",
    "        \n",
    "    def optimal_action(self) -> int:\n",
    "        return np.argmax(self.mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the bandit algorithm base class and may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "class BanditAlgorithmBase:\n",
    "    def select_action(self) -> int:\n",
    "        pass\n",
    "    \n",
    "    def update(self, action: int, reward: float):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the bandit experiment and may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "class Experiment:\n",
    "    def __init__(self, environment, bandit_algorithm):\n",
    "        self.environment = environment\n",
    "        self.bandit_algorithm = bandit_algorithm\n",
    "        \n",
    "    def run_experiment(self, T=100):\n",
    "        instant_regrets = np.zeros(T)\n",
    "        for t in range(0, T):\n",
    "            action = self.bandit_algorithm.select_action()\n",
    "            reward = self.environment.perform_action(action)\n",
    "            self.bandit_algorithm.update(action, reward)\n",
    "            \n",
    "            optimal_action = self.environment.optimal_action()\n",
    "            instant_regret = self.environment.expected_value(optimal_action) - self.environment.expected_value(action)\n",
    "            instant_regrets[t] = instant_regret\n",
    "        cumulative_regrets = np.cumsum(instant_regrets)\n",
    "        return (instant_regrets, cumulative_regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a function for repeated experiments with a provided bandit algorithm, averaging regret over the runs. It may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "def run_repeated_experiments(bandit_algorithm_class: BanditAlgorithmBase, T: int,  K: int, seed: int, env_class: type = Environment):\n",
    "    instant_regrets = []\n",
    "    cumulative_regrets = []\n",
    "    if env_class == Environment:\n",
    "        env_constructor = lambda seed: Environment(K, seed)\n",
    "    elif env_class == CropEnvironment:\n",
    "        env_constructor = lambda seed: CropEnvironment(seed)\n",
    "    else:\n",
    "        raise ValueError(f\"Argument env_class must be either 'Environment' or 'CropEnvironment', but got {env_class}.\")\n",
    "    for i in range(ITERATIONS):\n",
    "        bandit_algorithm = bandit_algorithm_class(T, K)\n",
    "        environment = env_constructor(seed+i+1)\n",
    "        experiment = Experiment(environment, bandit_algorithm)\n",
    "\n",
    "        instant_regrets_i, cumulative_regrets_i = experiment.run_experiment(T)\n",
    "        instant_regrets.append(instant_regrets_i)\n",
    "        cumulative_regrets.append(cumulative_regrets_i)\n",
    "    return pd.DataFrame(data={'t': np.arange(1, T+1),\n",
    "                             'instant_regret': np.mean(np.vstack(np.array(instant_regrets)), axis=0),\n",
    "                             'regret': np.mean(np.vstack(np.array(cumulative_regrets)), axis=0)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Bandits (Chapter 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Implement Explore-First\n",
    "(2 points, at most 6 points from implementations)\n",
    "\n",
    "Implement the *Explore-First* algorithm (**Algorithm 1.1** in [Bandits]) within the provided bandit algorithm template below. Use $N = \\left(\\frac{T}{K}\\right)^{2/3} \\cdot \\left( \\log T \\right)^{1/3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreFirst(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging the regret over all runs). The exploration and exploitation phases should be clearly visible in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "ef_df = run_repeated_experiments(ExploreFirst, T, K, SEED)\n",
    "ef_df.plot(x='t', y='regret', title='Explore-First')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Implement Epsilon-Greedy\n",
    "(2 points, at most 6 points from implementations)\n",
    "\n",
    "Implement the $ \\epsilon_t $-*Greedy* algorithm (**Algorithm 1.2** in [Bandits]) within the provided bandit algorithm template below. Use $\\epsilon_t = \\min \\left\\{1,\\ t^{-1/3} \\cdot (K \\log t)^{1/3}\\right\\} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTGreedy(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging the regret over all runs). The plot should show sublinear regret with respect to $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "eg_df = run_repeated_experiments(EpsilonTGreedy, K, SEED)\n",
    "eg_df.plot(x='t', y='regret', title='Epsilon_t-Greedy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Implement UCB1\n",
    "(2 points, at most 6 points from implementations)\n",
    "\n",
    "Implement the UCB1 algorithm (**Algorithm 1.5** in [Bandits]) within the provided bandit algorithm template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB1(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging the regret over all runs). The plot should show sublinear regret with respect to $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "ucb1_df = run_repeated_experiments(UCB1, T, K, SEED)\n",
    "ucb1_df.plot(x='t', y='regret', title='UCB1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Regret for modified UCB1\n",
    "(6 points) \n",
    "\n",
    "This theory problem is based on **Exercise 1.1** in [Bandits]. The proofs in **Chapter 1** consider environments where the rewards are in the interval $[0,1]$. Consider the case when we have additional knowledge about about the problem and that we know that the rewards for each action are in the interval $\\left[\\frac{1}{2}, \\frac{1}{2} + \\epsilon\\right]$ for some fixed $\\epsilon \\in \\left(0, \\frac{1}{2}\\right)$. \n",
    "\n",
    "Consider a version of $\\text{UCB1}$ modified to utilize this knowledge (you do not need to specify the algorithm completely, just define the new confidence radius $r_t(a)$). For this algorithm and problem setting, prove that:\n",
    "\n",
    "$\\mathbb{E}\\left[R(t)\\right] \\leq \\frac{2 \\epsilon t}{T^2} + 2 \\epsilon \\sqrt{2 K t \\log T}$\n",
    "\n",
    "**Instructions:** Use a version of Hoeffding Inequality with ranges (**Theorem A.2** in the [Bandits] book) to modify the confidence radius $r_t(a)$. Subsequently follow the steps of the analysis leading up to **Theorem 1.14** in [Bandits] to derive the regret bound, though show the actual constants instead of using big O notation:\n",
    "\n",
    "1. Define the clean event, like in **Section 1.3.1**, and lower bound the probability of the event.\n",
    "2. Start with the definition of the regret $\\mathbb{E}\\left[R(t)\\right]$, and perform a regret decomposition like on **Page 11** of **Section 1.3.2**.\n",
    "3. Bound the *gap* $\\Delta (a_t)$, like in **Section 1.3.3**.\n",
    "4. Complete the proof using the technique on **Page 12** of **Section 1.3.2**. Note that applying the bound in step 3 requires careful motivation.\n",
    "\n",
    "*Write the solution in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Bandits (Chapter 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Implement Thompson Sampling\n",
    "(2 points, at most 6 points from implementations)\n",
    "\n",
    "Implement the *Thompson Sampling* algorithm (**Algorithm 3.3** in [Bandits]) within the provided bandit algorithm template below. Assume independent priors and that the prior is $\\mathbb{P} = \\text{Beta}(\\alpha_0, \\beta_0)$ with $\\alpha_0 = 1$ and $\\beta_0 = 1$ (i.e. the **Beta-Bernoulli** setting, on **page 35** in [Bandits]).\n",
    "\n",
    "**Note:** There is a typo in the expression for the posterior $\\mathbb{P}_H$ in [Bandits]. It should be $\\text{Beta}(\\alpha_0 + \\text{REW}_H,\\ \\beta_0 + t - \\text{REW}_H)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging the regret over all runs). The plot should show sublinear regret with respect to $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "ts_df = run_repeated_experiments(ThompsonSampling, T, K, SEED)\n",
    "ts_df.plot(x='t', y='regret', title='Thompson Sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Regret for Thompson Sampling\n",
    "(6 points)\n",
    "\n",
    "In this theory problem, you will show an intermediary step in the proof for the Bayesian regret bound of *Thompson Sampling* in the [Bandits] book.\n",
    "\n",
    "You are given a $K$-armed bandit problem with rewards in the interval $[0, 1]$. You can assume that $K \\leq T$, where $T$ is the horizon. Additionally, you can assume that **Lemma 1.5** holds (i.e., for this assignment we define $r_t (a) := \\sqrt{\\frac{2  \\log T}{ n_t (a)}}$, and then it holds that $\\text{Pr}\\left\\{ \\mathcal{E} \\right\\} \\geq 1 - \\frac{2}{T^2}$ with $\\mathcal{E} := \\left\\{ \\forall a \\forall t \\;\\; \\vert \\bar{\\mu}_t (a) - \\mu (a) \\vert \\leq r_t (a) \\right\\}$). Then, with $\\text{UCB}_t (a) := \\bar{\\mu}_t (a) + r_t (a)$, show that $\\mathbb{E}\\left[ \\left[ \\text{UCB}_t (a) - \\mu (a) \\right]^{-} \\right] \\leq \\frac{2}{TK}$ (i.e., show that Equation 3.14 in [Bandits], with $\\gamma = 2$, holds for all arms $a$ and rounds $t$).\n",
    "\n",
    "**Note:** $[x]^{-}$ is the negative portion of $x$, i.e., $[x]^{-} = 0$ if $x \\geq 0$ and $[x]^{-} = \\vert x \\vert$ otherwise.\n",
    "\n",
    "**Hint:** Remember that, given a random variable $X$, an event $\\mathcal{E}$ (subset of the sample space) and its complement $\\mathcal{E}^c$, by the tower rule, $\\mathbb{E}\\left[ X \\right] = \\mathbb{E}\\left[ X \\;\\vert\\; \\mathcal{E} \\right] \\cdot \\text{Pr}\\left\\{ \\mathcal{E} \\right\\} + \\mathbb{E}\\left[ X \\;\\vert\\; \\mathcal{E}^c \\right] \\cdot \\text{Pr}\\left\\{ \\mathcal{E}^c \\right\\}$.\n",
    "\n",
    "*Write the solution in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall comparison\n",
    "The cell below plots the regret for all algorithms. Make sure to comment out the any algorithms you have not implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "fig.suptitle(\"Final comparison\")\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"Regret\")\n",
    "dfs = [ # Comment out any algorithm that you have not implemented.\n",
    "    (ef_df, \"Explore-First\"), \n",
    "    (eg_df, \"Epsilon_t-Greedy\"), \n",
    "    (ucb1_df, \"UCB1\"), \n",
    "    (ts_df, \"Thompson Sampling\")\n",
    "]\n",
    "for df, label in dfs: df.plot(x='t', y='regret',ax = ax, label = label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Multi-armed bandits for optimizing crop yields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6 points)\n",
    "\n",
    "We consider the problem of selecting a date for planting maize grains as a multi-armed bandit problem. Year-to-year crop yields can vary heavily and depend on soil quality, genetics, the weather and other factors. We wish to identify the best date to plant maize among 7 dates. To help us, we will use data from a detailed simulator that generates realistic crop yields.\n",
    "\n",
    "You have been given samples from $K = 3$ dates to experiment with locally in `samples.pkl`. The crop yields have been rescaled to lie in $[0,1]$ but the distributions do not fit most parametric models, see the histogram of one of the arms below.\n",
    "\n",
    "![hist](sample_histogram.png?1)\n",
    "\n",
    "When you submit this notebook to Canvas, your solution will be automatically evaluated on the full dataset of 7 dates. If the evaluation successfully ran, you will receive a comment in Canvas indicating your average total regret. If the evaluation failed to run, you should hopefully receive an error message on your submission. If you do not receive an error message or do not understand the error message, feel to reach out to the responsible TA or visit the consultation sessions. Your submission will be evaluated on 100 runs and must finish under 15 minutes. If your submission exceeds the time limit, you will likely not receive a comment.\n",
    "\n",
    "The best result for each group will be tracked and uploaded to a [scoreboard on Canvas](https://chalmers.instructure.com/courses/35974/pages/scoreboard) where you will also see the three baselines that you must beat to earn points on this assignment. **Each baseline that you beat yields 2 points, up to a total of 6 points.** In addition, you must describe the process of solving this problem. Your description could include: the algorithms you attempted initially, why you think they worked poorly for this problem, what you tried to improve your algorithm (regardless of success) and a description of the final submission. Descriptions of low quality may reduce your points on this problem.\n",
    "\n",
    "In order to evaluate your submission, one cell in the notebook must contain the string `SUBMISSION ALGORITHM` on the **first line** and must contain a class declaration that creates a subclass of `BanditAlgorithmBase`, see the example of RandomPolicy below.\n",
    "**Important to note:** Your class *must not* depend on any external or global variables not provided to it, e.g. the line `self.K = K_CROPS` in `__init__` is not allowed but `self.K = K` is since `K` is a local variable. Similarly, your class *must only* depend on the Python packages provided in the setup cell (changing the setup cell does not affect our evaluation script).\n",
    "\n",
    "**Hint:** Consider the information you are given about the reward distribution and the assumptions each algorithm assumes of the reward distributions. Which algorithm's assumptions most closely matches (or differs the most from) the information given about the reward distribution?\n",
    "\n",
    "**Hint:** Some of the algorithms implemented in the previous problems contain hyperparameters that can be tuned to improve performance. Your solutions to previous problems must still match the descriptions in [Bandits]. If you do optimize the hyperparameters, implement a new class below.\n",
    "\n",
    "**Hint:** The multi-armed bandit literature has plenty of algorithms that could be of interest here, see for example the book by [Lattimore](https://tor-lattimore.com/downloads/book/book.pdf), Successive Elimination (Algorithm 1.4 in [Bandits]), [RandUCB](https://arxiv.org/pdf/1910.04928), [KL-UCB](https://arxiv.org/pdf/1102.2490), Thompson sampling with Gaussian priors, [Feel-Good Thompson sampling](https://arxiv.org/pdf/2110.00871)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of your process and final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMISSION ALGORITHM\n",
    "class RandomPolicy(BanditAlgorithmBase): # Feel free to update the name of the policy to better describe your solution.\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        self.K = K\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        return np.random.choice(self.K)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "policy = RandomPolicy\n",
    "submission_df = run_repeated_experiments(policy, T, K_CROPS, SEED, CropEnvironment)\n",
    "submission_df.plot(x = 't', y = 'regret', title = f'Crop Regret ({policy.__name__})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banditassignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
